# Blue Bikes Station Demand Prediction Project - Final Report
**Video Presentation:** []

## Reproducibility, Setup and Data Download Guide

This project uses a `Makefile` to automate environment setup and dependency installation. However, datasets must still be downloaded and placed into the correct folders manually. Follow these steps to reproduce the results:

### Dataset Requirements (download instructions below)

The following data files are required:
- **Trip data:** 45 monthly CSV files (Jan 2022 - Sep 2025) in `data/raw/trips/{year}/`
- **Station list:** `data/raw/stations/-External-_Bluebikes_Station_List - current_bluebikes_stations (2).csv`
- **Weather data:** `data/raw/weather/open-meteo-42.36N71.13W19m (2).csv`
- **Calendar features:** Auto-generated by `make data` ‚Üí `data/raw/dates/calendar_features.csv`

### Download Data and Folder Placement Instructions

#### **Option 1: Quick Download (Recommended for Graders)**

All required datasets are available in a single Google Drive folder for easy access:

üìÅ **[Download All Data from Google Drive](https://drive.google.com/drive/folders/1d2kYZuG_JKQV1iDsEM0z4ysgQemJL-rO?usp=drive_link)**

**Instructions:**
1. Access the Google Drive folder using the link above
2. Download all files and folders
3. Place the downloaded files into your local project directory following this structure:
   ```
   data/raw/
   ‚îú‚îÄ‚îÄ trips/
   ‚îÇ   ‚îú‚îÄ‚îÄ 2022/  (12 CSV files)
   ‚îÇ   ‚îú‚îÄ‚îÄ 2023/  (12 CSV files)
   ‚îÇ   ‚îú‚îÄ‚îÄ 2024/  (12 CSV files)
   ‚îÇ   ‚îî‚îÄ‚îÄ 2025/  (9 CSV files)
   ‚îú‚îÄ‚îÄ stations/
   ‚îÇ   ‚îî‚îÄ‚îÄ -External-_Bluebikes_Station_List - current_bluebikes_stations (2).csv
   ‚îî‚îÄ‚îÄ weather/
       ‚îî‚îÄ‚îÄ open-meteo-42.36N71.13W19m (2).csv
   ```
4. The `calendar_features.csv` file will be auto-generated when you run `make all`

---

#### **Option 2: Original Data Collection Process I Used (For Reference)**

<details>
<summary>Click to expand detailed instructions for collecting data from original sources</summary>

These are the steps I originally followed to collect the datasets. Note that some sources may no longer be available.

##### 1. Trip Data (45 CSV files)
1. Go to [Blue Bikes System Data](https://www.bluebikes.com/system-data)
2. Click "Download Bluebikes trip history data"
3. Download all monthly files from **January 2022 to September 2025**:
   - Files: `202201-bluebikes-tripdata.zip` through `202509-bluebikes-tripdata.zip` (45 files total)
4. Unzip each file to extract the CSV (e.g., `202201-bluebikes-tripdata.csv`)
5. **Place files by year:**
   - 2022 CSVs ‚Üí `data/raw/trips/2022/`
   - 2023 CSVs ‚Üí `data/raw/trips/2023/`
   - 2024 CSVs ‚Üí `data/raw/trips/2024/`
   - 2025 CSVs ‚Üí `data/raw/trips/2025/`

##### 2. Station List (1 CSV file)
‚ö†Ô∏è **IMPORTANT:** The official station list from August 2025 is no longer available on the Blue Bikes website.

**Download from archived copy:**
1. Go to [Google Sheets backup](https://docs.google.com/spreadsheets/d/1wmt_GaCWRFRbg7ZpADuwTdy3EI54ToK7QJKgBt5ICnM/edit?usp=sharing)
2. Download as CSV
3. Place in `data/raw/stations/`

**Original download method (no longer works):**
1. Go to [Blue Bikes System Data](https://www.bluebikes.com/system-data)
2. Find "Bluebikes Stations" section
3. Click "Bluebikes Station List"
4. Download CSV

##### 3. Weather Data (1 CSV file)
1. Go to [Open-Meteo Historical Weather API](https://open-meteo.com/en/docs/historical-weather-api)
2. Configure **Location and Time** settings:
   - Match filters shown in ![Weather Filter 1](visualizations/weather_data_filter1.png)
3. **Hourly Weather Variables:** Uncheck all
4. **Daily Weather Variables:** Check filters shown in ![Weather Filter 2](visualizations/weather_data_filter2.png)
5. Scroll to "API Response" and click **"Download CSV"**
6. Place in `data/raw/weather/`

##### 4. Calendar Data (Auto-generated)
‚úÖ **No manual download required** - automatically generated by `make data`

The script (`scripts/getCalendarFeatures.py`) generates `calendar_features.csv` containing:
- Federal holidays (via Python `holidays` module)
- BU academic breaks (manually encoded from [BU Academic Calendar](https://www.bu.edu/reg/calendars/))
- Day of week indicators (0=Monday, 6=Sunday)

</details>

### Run Makefile Setup 

```bash
# 1. Clone the repository
git clone https://github.com/julianl05/506-final-project.git
cd 506-final-project

# 2. Run complete setup (creates venv, installs dependencies, runs calendar data generation script, validates datasets exist and are placed in correct locations)
make all

# 3. Activate the virtual environment
source venv/bin/activate

# 4. Open VS Code and run notebooks in order (ensure venv is selected as environment for kernel):
#    - notebooks/01_process_data.ipynb
#    - notebooks/02_visualisations_and_feature_extraction.ipynb
#    - notebooks/03_modeling.ipynb
```

### Makefile Commands

```bash
make help          # Show all available commands
make setup         # Create virtual environment and install dependencies
make data          # Generate calendar features CSV
make check-data    # Validate all required data files exist
```

### System Requirements
- **macOS users:** Requires OpenMP for XGBoost
  ```bash
  brew install libomp
  ```
- **Operating System:** macOS, Linux, or Windows with WSL. Makefile is not setup to work with Windows.
---
## Introduction

This project's goal is to predict daily trip demand at Boston Blue Bikes stations, i.e. estimating how many trips a bluebike station will have on a given date. Ideally, this would help blue bike users plan their trips in advance by knowing which stations will be busy and which won't be. Personally, I've found myself in numerous situations where I planned a trip the night before assuming there would be bikes available at the nearest station, only to wake up the next morning to find none available. This project aims to make bike availability more predictable.

**Hypothesis:** I treat this as a regression problem, predicting daily trip counts based on multiple feature types. Based off personal experience, I believe weather, time(calendar/holidays) and historical trip data would affect trip counts the most, and thus would make the best features. I explore these relationships through visualisations and feature analysis that will be shown in this report.

**Data Sources:** Historical trip data from the Blue Bikes system data portal (https://www.bluebikes.com/system-data), daily weather data from the Open-Meteo Historical Weather API (https://open-meteo.com/en/docs/historical-weather-api), federal holidays from Python's holidays module, and BU's academic calendar (https://www.bu.edu/reg/calendars/) for academic holiday data.

**Modeling Strategy:** My current plan is to start with a baseline linear regression model to establish benchmark performance, then plan to implement more complex methods (like XGBoost) for the final report to capture non-linear relationships. As of right now, only the linear regression model has been implemented and evaluated.

**Evaluation:** The model is trained on 2022-2024 data and tested on 2025 data to ensure it generalizes to future dates. Performance is measured using RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), and R¬≤ (coefficient of determination), as well as directly comparing predicted vs actual results.



---

### 1. Data Collection and Processing

Getting the data ready for modeling turned out to be more complex than expected. Here's what I dealt with:

#### Starting Point: Raw Data Sources
All datasets range from 1-1-2022 to 9-30-2025

**Blue Bikes Trip Data (45 CSV files, Jan 2022 to Sep 2025 inclusive):**
- Individual trip records with start/end times, station IDs, and user types
- Initial Format: One row per trip (15.7M trips total)
- Challenge discovered: CSV format changed mid-way through our data range because bluebikes updated their system in April 2023

**Weather Data (Open-Meteo API):**
- Daily weather observations for Boston coordinates (42.36¬∞N, 71.13¬∞W)
- Variables: temperature (min/max/mean), precipitation, wind speed, snowfall
- Initial Format: One row per day

**Calendar Data (Generated manually using getCalendarFeatures.py):**
- Federal holidays from 2022-2025 (New Year's, MLK Day, Memorial Day, etc.)
- BU Academic calendar breaks (Winter break, Spring break, Summer, Thanksgiving)
- Format: Generated CSV using Python's holidays and pandas modules with `date`, `is_holiday`, `day_of_week`, `is_academic_break` columns

#### Challenge 1: CSV Format Changed Mid-Dataset

When loading the trip files, I discovered Blue Bikes changed their data format in April 2023:

**Old Format (Jan 2022 - Mar 2023):**
```
tripduration,starttime,stoptime,start station id,start station name,usertype
597,"2022-01-01 00:00:25.1660","2022-01-01 00:10:22.1920",178,"MIT Pacific St",Subscriber
```

**New Format (Apr 2023 - Sep 2025):**
```
ride_id,started_at,ended_at,start_station_id,start_station_name,member_casual
"ABC123","2023-04-13 13:49:59","2023-04-13 13:55:10","M32006","MIT at Mass Ave",member
```

**Solution:** Detect which format each file uses and standardize all columns to the new format during loading. This lets me combine all 45 monthly files into one dataset.

#### Challenge 2: Station IDs Changed Too

Not only did the CSV format change, but Blue Bikes also switched their station ID system:
- **Old system:** Numeric IDs (67, 178, 486)
- **New system:** Alphanumeric IDs (M32006, D32016, N32008)

The same physical station location had different IDs in the two periods! For example:
- Historic ID `67` and new ID `M32006` are both "MIT at Mass Ave / Amherst St"
- Historic ID `178` and new ID `M32041` are both "MIT Pacific St at Purrington St"

This caused duplicate counting - we initially saw 627 "unique" stations when there are actually only 608 physical locations.

**Solution:** 
1. Downloaded official Blue Bikes station list which includes a "Station ID (to match to historic system data)" column
2. Created a mapping dictionary from old IDs to new IDs (443 mappings)
3. Converted all old IDs in the trip data to their new equivalents
4. Final result: 608 unique stations correctly identified
Note: the official station list also showed that there were 573 stations still in use, meaning 608-573 = 35 stations no longer in use. 


#### Challenge 3: Station Name Duplicates

Even after ID mapping, some duplicate stations still remained because they were no longer listed in the official station list (out of order). 

**Solution:** For each unique station name, consolidate to a single preferred ID (favoring the new format alphanumeric IDs over numeric ones), then re-aggregate trip counts.

#### Processing Steps
Primarily used pandas Python module for processing and cleaning. See detailed workflow in the process_data.ipynb <br/>
**1. Trip Data Aggregation:**
- Loaded all 45 monthly CSVs with 15.7 million trips (rows)
- Standardized columns and station IDs (challenges faced detailed above)
- Extracted date from timestamps
- Aggregated from individual trips to total trips per station per day
- Result: CSV with 566,622 rows detailing for every date/station pair, how many trips a station had on that date. 

**2. Weather Data Processing:**
- Loaded CSV, skipped metadata rows at the top
- Renamed columns (removed units, simplified names)
- Converted date column to datetime
- Extracted key features: `temp_mean`, `precipitation`, `wind_speed`, `snowfall`
- Result: 1,369 daily weather values

**3. Calendar Features:**
- Created CSV manually with federal holidays (44 days) using getCalendarFeatures.py script
- For academic breaks, used BU academic calendar (599 days across all breaks)
- Included `day_of_week` (0=Monday, 6=Sunday) for each date
- Result: 1,461 days with calendar features

**4. Merging:**
- Started with daily trip counts per station
- Left join weather data on date (all station-day records get same weather)
- Left join calendar features on date
- Added temporal features: `month`, `year` extracted from dates
- Result: Single merged dataset ready for modeling

#### Final Dataset Structure

**Before (Individual Trip Record):**
```
ride_id,started_at,ended_at,start_station_id,start_station_name,member_casual
"ABC123","2025-01-03 20:24:01","2025-01-03 20:28:53","D32000","Cambridge St at Joy St","member"
```

**After (Daily Station Aggregation):**
```
station_id,start_station_name,date,trip_count,lat,lng,temp_mean,precipitation,wind_speed,snowfall,is_holiday,day_of_week,is_academic_break,month,year
"D32000","Cambridge St at Joy St","2025-01-03",45,42.361,-71.065,8.2,2.2,10.2,0.0,0,4,1,1,2025
```

**Final Processed and Cleaned Dataset Specs:**
- **Rows:** 566,622 (station-day combinations)
- **Columns:** 15 features
- **Date Range:** 2022-01-01 to 2025-09-30
- **Stations:** 608 unique locations
- **Total Trips Represented:** 15,767,696

---

### 2. Exploratory Data Analysis (Visualisations and Feature Analysis)

Now that I had clean data, I wanted to understand what actually drives bike demand so I could determine if the features I chose were actually relevant. I did this by creating lots of visualisations and doing correlation analysis with the features in the processed dataset.

**Usage Patterns: Day of Week √ó Month**

![Day of Week vs Month Heatmap](visualizations/day_week_month_heatmap.png)

This heatmap reveals peak usage times across the entire system. The darkest cells show the busiest combinations:
- **Peak period:** Weekdays (Mon-Fri) in summer/fall months (June-September)
- **Lowest period:** Weekends in winter months (December-February)
- **Key insight:** September weekdays are the absolute peak (likely due to university students returning + still-warm weather)

The heatmap confirms that both temporal factors (day of week AND month) matter simultaneously.

**Geographic Distribution:**

![Station Activity Map](visualizations/station_activity_map.png)

The busiest stations cluster around MIT, Harvard, and Central Square - university areas with high commuter traffic. The top station (MIT at Mass Ave) averages 229 daily trips, while smaller suburban stations see around 5 trips per day.

**Case Study: MIT Station**

I picked the busiest station to examine patterns in detail:

![MIT Station Analysis](visualizations/case_study_station.png)

This four-panel view shows:
- **Top-left:** Clear seasonality over time (summer peaks, winter valleys)
- **Top-right:** Strong positive temperature correlation (warmer = more trips)
- **Bottom-left:** Weekdays are consistently busier than weekends
- **Bottom-right:** September is the peak month (likely students returning)

Do these patterns hold across all 608 stations? Let's check:

![System-Wide Analysis](visualizations/system_wide_analysis.png)

This system-wide view aggregates all stations and shows:
- **Top-left:** Same seasonal pattern (system-wide ridership peaks in summer)
- **Top-right:** Temperature correlation holds at system level (warmer weather = more total trips)
- **Bottom-left:** Weekday pattern confirmed (Thursdays and Fridays are busiest system-wide)
- **Bottom-right:** September peak appears across entire network (not just MIT)

**Comparison:**

| Metric | MIT Station | System-Wide Average |
|--------|-------------|---------------------|
| Average daily trips | 229.0 | 27.8 per station |
| Peak day of week | Friday | Friday |
| Peak month | September | September |
| Weekday/Weekend ratio | 1.15x | 1.11x |

The patterns at MIT station match system-wide trends, suggesting my findings generalize across the network. MIT is just a higher-volume version of the typical pattern. This is promising, as it means fitting a model to system wide data might make sense.

**Feature Correlation Analysis:**

![Feature Correlations](visualizations/feature_correlations.png)

**Linear correlations with trip count:**
- **Temperature** (r=0.21): Strongest positive predictor
- **Month** (r=0.13): Seasonal effect
- **Precipitation** (r=-0.08): Negative impact
- **Snowfall** (r=-0.07): Negative impact
- **Day of week** (r=-0.01): Surprisingly weak linear correlation

Some of these correlations seem weak, but that's because they're measuring *linear* relationships. Let's look at the actual patterns:

![Feature Scatter Grid](visualizations/feature_scatter_grid.png)

This grid shows how each feature relates to trip counts:
- **Temperature:** Clear positive relationship (not perfectly linear - optimal range exists)
- **Precipitation:** Non-linear decay (exponential drop-off with heavy rain)
- **Wind/Snow:** Negative relationships but lots of scatter
- **Day of week:** Categorical pattern (can't be captured by linear correlation)
- **Month:** Categorical seasonality (peaks in summer months)

**Non-Linear and Categorical Relationships:**

![Feature Relationships - Categorical](visualizations/feature_relationships_categorical.png)

This view better captures the true relationships:
- **Precipitation bins:** Shows exponential decay - heavy rain (10+ mm) reduces trips by ~45%
- **Temperature bins:** Optimal range 15-25¬∞C for maximum ridership
- **Day of week:** Weekdays consistently higher than weekends (this pattern was hidden in the weak r=-0.01 correlation!)
- **Month:** Clear seasonal pattern with September peak
- **Holidays:** Reduce trips by ~24%
- **Academic breaks:** Reduce trips by ~25%

**Key Finding:** Linear correlation coefficients underestimate the true predictive power of categorical and non-linear features. Day of week and month have strong effects that aren't captured by simple correlation.

**Summary of Feature Effects:**
- **Temperature:** Strong positive effect (r=0.21), optimal range 15-25¬∞C
- **Precipitation:** Exponential negative impact (~45% drop in heavy rain)
- **Seasonality:** 60% higher ridership in summer vs winter
- **Day of week:** 11% higher on weekdays (hidden by weak correlation)
- **Holidays/breaks:** ~24-25% reduction in ridership

---

### 3. Feature Engineering and Selection

After completing the exploratory data analysis, I conducted a systematic feature engineering and selection process to determine which features to include in the final model. This section documents my methodology and justification for each feature choice.

#### 3.1 Feature Engineering Process

**Step 1: Identify Candidate Features**

Based on the data collection phase and domain knowledge about bike-sharing systems, I identified three main categories of potential predictive features:

1. **Temporal Features** (from date information):
   - `day_of_week` (0=Monday, 6=Sunday)
   - `month` (1-12)
   - `year` (2022-2025)
   - `is_weekend` (derived from day_of_week)

2. **Weather Features** (from Open-Meteo API):
   - `temp_mean` (daily average temperature in ¬∞C)
   - `precipitation` (daily rainfall in mm)
   - `wind_speed` (daily average in km/h)
   - `snowfall` (daily snowfall in cm)

3. **Calendar Features** (from holidays module + BU calendar):
   - `is_holiday` (federal holidays, binary 0/1)
   - `is_academic_break` (BU breaks: winter, spring, summer, binary 0/1)

4. **Location Features**:
   - `station_id` (608 unique stations)
   - `lat`, `lng` (station coordinates)

**Step 2: Evaluate Feature Relevance Through Visualization**

I created comprehensive visualizations (Section 2) to understand each feature's relationship with trip counts:

- **Correlation analysis** revealed temperature (r=0.21) as the strongest linear predictor
- **Categorical analysis** showed day_of_week and month have strong effects despite weak linear correlation (r‚âà0.01)
- **Binned analysis** uncovered non-linear relationships (e.g., precipitation's exponential decay)

**Step 3: Feature Selection Criteria**

I selected features based on **two criteria combined**:

1. **Statistical Correlation**: Features showing meaningful correlation with trip counts (threshold: |r| > 0.02)
2. **Theoretical Relevance**: Features with clear causal relationships even if linear correlation is weak

This dual-criteria approach was necessary because some features (like `day_of_week`) exhibit strong **categorical patterns** rather than linear relationships. The visualizations in Section 2 revealed:

| Feature | Linear Correlation | Categorical Effect | Decision |
|---------|-------------------|-------------------|----------|
| `temp_mean` | r=0.21 (strong) | 60% increase in optimal range | ‚úÖ **Include** |
| `precipitation` | r=-0.08 (moderate) | 45% decrease in heavy rain | ‚úÖ **Include** |
| `wind_speed` | r=-0.05 (weak) | Negative but noisy relationship | ‚úÖ **Include** (weather completeness) |
| `snowfall` | r=-0.07 (moderate) | Strong negative in winter | ‚úÖ **Include** |
| `day_of_week` | r=-0.01 (very weak) | **11% higher on weekdays** | ‚úÖ **Include** (categorical pattern) |
| `month` | r=0.13 (moderate) | **60% seasonal variation** | ‚úÖ **Include** (categorical pattern) |
| `is_holiday` | r=-0.05 (weak) | **24% reduction** on holidays | ‚úÖ **Include** (binary pattern) |
| `is_academic_break` | r=-0.04 (weak) | **25% reduction** during breaks | ‚úÖ **Include** (binary pattern) |
| `station_id` | N/A | 229 vs 5 trips/day range | ‚úÖ **Include** (baseline demand) |

**Key Insight from Analysis**: Features like `day_of_week` and `is_academic_break` show low **numerical linear correlation** but their **visualizations reveal strong categorical correlations**. The bar charts in Section 2 clearly demonstrate weekdays consistently outperform weekends, and non-break periods have higher ridership. These patterns justify inclusion despite weak Pearson correlation coefficients.

#### 3.2 Feature Engineering Decisions

**Decision 1: One-Hot Encoding for Station ID**

Station location emerged as the dominant predictor. Different stations have vastly different baseline demand:
- MIT station: 229 average daily trips
- Suburban stations: ~5 average daily trips

I tested the model with and without `station_id`:
- **Without station_id**: R¬≤ = 0.048 (model essentially useless)
- **With station_id**: R¬≤ = 0.731 (captures 73% of variance)

**Implementation**: One-hot encoded `station_id` into 608 binary dummy variables, allowing each station to learn its own baseline demand level. This treats station location as a categorical feature rather than incorrectly implying ordering.

**Decision 2: Keep Temporal Features Despite Weak Linear Correlation**

While `day_of_week` shows r=-0.01 correlation, the categorical analysis (Section 2, Feature Relationships - Categorical plot) reveals:
- Weekdays average 11% higher ridership than weekends
- Effect is consistent across all 608 stations
- Pattern holds across all seasons

**Justification**: The weak linear correlation occurs because `day_of_week` (0-6 encoding) has no linear relationship with trip counts, but the **categorical bins** show clear patterns. A model can learn "Monday=30 trips, Sunday=27 trips" even if the numeric encoding doesn't correlate linearly.

**Decision 3: Include All Weather Variables**

Even though `wind_speed` has weak individual correlation (r=-0.05), weather conditions work **in combination**:
- Heavy rain + high wind = major ridership drop
- Cold temperature + snow = winter minimum
- Warm + dry = summer maximum

Including all weather variables allows the model to capture these interaction effects.

**Decision 4: Binary Calendar Flags**

Holidays and academic breaks show ~24-25% ridership reductions (Section 2 categorical analysis). These are **discrete events** best represented as binary flags rather than continuous variables.

#### 3.3 Final Feature Set

Based on this systematic evaluation, I selected **9 input features** for modeling:

| Category | Features | Count | Encoding |
|----------|----------|-------|----------|
| Weather | `temp_mean`, `precipitation`, `wind_speed`, `snowfall` | 4 | Continuous |
| Temporal | `day_of_week`, `month`, `is_academic_break` | 3 | Categorical/Binary |
| Calendar | `is_holiday` | 1 | Binary |
| Location | `station_id` | 1 (‚Üí608 dummies) | One-hot encoded |
| **Total** | | **9** | **616 features after encoding** |

**Features Excluded and Why**:
- `year`: Trend-based feature risks overfitting to training period; model should generalize to future years
- `lat`/`lng`: Continuous coordinates less interpretable than station_id dummies; ID captures location implicitly
- `is_weekend`: Redundant with `day_of_week`; can be derived if needed
- Individual `temp_min`/`temp_max`: `temp_mean` captures temperature effect with less multicollinearity

This feature set balances **statistical evidence** from correlation analysis, **domain knowledge** about bike-sharing demand drivers, and **practical considerations** like interpretability and avoiding overfitting.

---

### 4. Modeling Implementation

**Model Choice:** Linear Regression (baseline)

I started with a simple linear regression model as a baseline. This gives me a benchmark to beat with more complex models later. You can see my detailed implementation in `linear_modeling.ipynb`.

**Features Used:**

As determined through the systematic feature selection process in Section 3, I used the final 9 input features:
- **Weather (4 features):** `temp_mean`, `precipitation`, `wind_speed`, `snowfall`
- **Temporal (3 features):** `day_of_week`, `month`, `is_academic_break`
- **Calendar (1 feature):** `is_holiday`
- **Location (1 feature):** `station_id` (one-hot encoded into 608 dummy variables)

**Total Input Dimensionality:** 616 features (8 continuous/binary + 608 station dummies)

**Why Station ID?**

As demonstrated in Section 3.2 Decision 1, this was a critical feature engineering choice. Different stations have vastly different baseline demand - MIT averages 229 trips/day while small stations see 5 trips/day. 

**Ablation Study Results:**
- **Without station_id:** R¬≤ = 0.048 (explains only 4.8% of variance - essentially useless)
- **With station_id:** R¬≤ = 0.731 (captures 73% of variance - 15x improvement)

Station location is the single most important predictor. By one-hot encoding it, each station gets its own baseline adjustment, allowing the model to learn "MIT is typically busy, suburban station X is typically quiet" before applying weather/temporal effects.

**Train/Test Split:**

- **Train:** 2022-2024 data (432,022 rows)
- **Test:** 2025 data (134,600 rows)

---

### 5. Model Evaluation and Results
**Evaluation Approach:**

To evaluate our model, we use three standard regression metrics on both training (2022-2024) and test (2025) sets:
- **RMSE (Root Mean Squared Error):** Measures average prediction error, penalizing large errors more heavily. Useful for understanding worst-case performance.
- **MAE (Mean Absolute Error):** Average absolute difference between predicted and actual trips. More interpretable than RMSE - directly tells us "off by X trips on average."
- **R¬≤ (Coefficient of Determination):** Proportion of variance explained by the model (0 = useless, 1 = perfect). Shows how much better the model is than just predicting the mean.

By comparing training vs test performance, we can detect overfitting (model memorizing training data rather than learning generalizable patterns). Similar scores indicate the model generalizes well to future dates.

**Model Performance:**

| Metric | Training | Test |
|--------|----------|------|
| RMSE | 17.69 | 16.74 |
| MAE | 11.02 | 11.67 |
| R¬≤ | 0.752 | 0.731 |

**What this means:**
- Model explains 73.1% of variance in daily trip counts (pretty good for a baseline!)
- Average prediction error is ¬±12 trips
- Test performance nearly matches training (no overfitting)

**Sample Predictions:**

I wanted to see how the actual predicted values fare, so I looked at the model's predictions for the MIT station we did a case study on when analysing features.

| Date | Actual Trips | Predicted | Error |
|------|--------------|-----------|-------|
| 2025-09-19 | 444 | 249 | +195 |
| 2025-09-05 | 434 | 248 | +186 |
| 2025-09-20 | 417 | 243 | +174 |

The model underpredicts these high-traffic days. Here's the full time series:

![Predictions vs Actual](visualizations/predictions_vs_actual.png)

Notice how the predictions (red dashed line) capture the general level but miss the day-to-day spikes and dips. The model is predicting "typical demand" but can't account for unusual events.

**Where the Model Works Well:**
- Median error is only 8.7 trips
- Typical days are predicted accurately
- Captures seasonal trends (predictions rise from winter to fall)
- Generalizes well to 2025 (unseen data)

**Where It Struggles:**
- Can't predict unusual spikes (concerts, events, first day of classes)
- Smooths over day-to-day volatility
- Misses extreme weather impacts (not just linear relationships)

**Feature Impact Analysis:**

To validate the feature selection decisions from Section 3, I calculated how much each feature actually moves predictions across its observed range in the test set:

| Feature | Impact on Predictions | Validation Against EDA |
|---------|----------------------|------------------------|
| Temperature | ¬±59 trips (coldest to hottest days) | ‚úÖ Matches Section 2 finding: strong positive effect |
| Precipitation | -34 trips (dry to heavy rain) | ‚úÖ Matches Section 2 finding: exponential decay |
| Month | ¬±3 trips (January to December) | ‚ö†Ô∏è Lower than expected from 60% seasonal variation |
| Day of week | ¬±2 trips (Monday to Sunday) | ‚ö†Ô∏è Lower than expected from 11% weekday effect |
| Holidays | -7 trips | ‚úÖ Matches Section 2 finding: 24% reduction |
| Academic breaks | -7 trips | ‚úÖ Matches Section 2 finding: 25% reduction |

**Key Finding - Feature Interaction Effect:** 

Month and day_of_week have surprisingly small individual effects (¬±2-3 trips) even though Section 2 EDA showed strong patterns (60% seasonal variation, 11% weekday effect). 

**Explanation**: The station_id feature dominates and captures much of the temporal variation. Each station learns "I'm typically busy/quiet" as a baseline, which absorbs some of the day_of_week and month effects. For example:
- MIT station gets coefficient ‚âà+200 (high baseline)
- Suburban station gets coefficient ‚âà+5 (low baseline)
- Then day_of_week adds only ¬±2 trips on top of that baseline

The model knows "MIT is busy" but doesn't strongly distinguish "MIT on Monday vs MIT on Saturday." This suggests the temporal features work differently than in the EDA where we aggregated across all stations.

**Implication for Feature Selection**: Even though individual temporal feature coefficients are small, they were still worth including because:
1. They provide consistent improvements across all 608 stations
2. Combined effect of all temporal features is larger than individual effects
3. Without them, model cannot capture any day-to-day or seasonal variation

This validates the Section 3 decision to include features based on both correlation AND theoretical relevance, not just correlation alone.

---

### 6. Limitations and Future Work

#### Current Limitations

**The Problem:**

Linear regression with station dummies achieves good overall accuracy (R¬≤=0.731) but has several limitations:

1. **Smooth predictions**: Can't capture day-to-day volatility. Good for comparing relative demand between stations, but not precise for exact daily counts.

2. **Feature interaction masking**: Station dummies dominate, making other feature effects appear smaller than their true impact (as seen in Section 5 Feature Impact Analysis).

3. **Linear assumption**: Assumes linear relationships even though Section 2 showed features like precipitation have exponential decay patterns.

4. **Limited station-specific patterns**: All stations share the same coefficients for weather/temporal features. The model can't learn "rainy days hurt MIT more than suburban stations" or "weekday effect is stronger at commuter stations."

#### Proposed Improvements

**Approach 1: XGBoost (Gradient Boosted Trees)**

For the final report, I plan to test XGBoost to address the limitations above.

**Expected advantages over linear regression:**

1. **Station-specific patterns**: Trees can split on station_id first, then learn different patterns per station. For example:
   - At MIT specifically: weekdays have 40+ more trips than weekends
   - At suburban stations: weekday effect might be minimal
   - Linear regression forces all stations to share the same 2.5-trip day_of_week coefficient

2. **Non-linear effects**: Trees naturally capture:
   - Exponential precipitation decay (seen in Section 2)
   - Optimal temperature ranges (15-25¬∞C peak seen in Section 2)
   - Interaction effects (rainy Monday in January at MIT vs sunny Friday in September)

3. **Better volatility capture**: Can model complex combinations of features rather than assuming additive effects.

4. **Feature importance validation**: Will provide another perspective on feature selection decisions from Section 3.

**Expected outcome**: R¬≤ improvement to 0.75-0.80, better handling of unusual days, station-specific feature importance rankings.

**Approach 2: Per-Station Models (Alternative)**

Train 608 separate linear models (one per station) so each location gets its own coefficients for all features. 

**Trade-offs**:
- ‚úÖ Each station learns its own weekday effect, weather sensitivity, etc.
- ‚úÖ Computationally simple (just run 608 linear regressions)
- ‚ùå Low-traffic stations have limited data (may overfit)
- ‚ùå Can't share patterns between similar stations
- ‚ùå 608√ó more coefficients to interpret

This could work well for high-traffic stations (MIT, Harvard, Central Square) but struggle with suburban stations that have sparse data.

#### Reflection on Feature Engineering Process

The systematic feature selection process (Section 3) proved valuable:
- **Visualization first**: EDA in Section 2 revealed categorical patterns missed by correlation alone
- **Dual criteria**: Using both correlation AND theoretical relevance prevented excluding important features like day_of_week
- **Ablation testing**: Station_id test (R¬≤ 0.048 ‚Üí 0.731) quantified its critical importance
- **Validation**: Section 5 analysis confirmed most features work as expected, with station_id interaction explaining temporal feature coefficient sizes

For future work, I would also consider:
- **Time-lagged features**: Yesterday's trips might predict today's demand
- **Moving averages**: 7-day rolling average to capture trends
- **Event data**: Concerts, sports games, conferences (if available)
- **Real-time availability**: Number of bikes currently at station (for operational predictions)

---